"""
Whisperè¯­éŸ³è¯†åˆ«å¤„ç†å™¨æ¨¡å—
ä½¿ç”¨OpenAI Whisperè¿›è¡Œè¯­éŸ³è¯†åˆ«å’Œè½¬å½•
"""

import os
import json
import logging
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple
from .utils import validate_file_path, create_output_dir, safe_filename
from .punctuation_segment_optimizer import PunctuationSegmentOptimizer
from .semantic_segmenter import SemanticSegmenter
from .output_manager import OutputManager, StepNumbers
import math

try:
    import soundfile as sf
except Exception:
    sf = None

# å°è¯•å¯¼å…¥åŸç”Ÿ Whisper
try:
    import whisper
    WHISPER_AVAILABLE = True
except ImportError:
    WHISPER_AVAILABLE = False

# å°è¯•å¯¼å…¥ Faster-Whisper
try:
    from faster_whisper import WhisperModel as FasterWhisperModel
    FASTER_WHISPER_AVAILABLE = True
except ImportError:
    FASTER_WHISPER_AVAILABLE = False


class WhisperProcessor:
    """Whisperè¯­éŸ³è¯†åˆ«å¤„ç†å™¨ç±»"""
    
    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–Whisperå¤„ç†å™¨
        
        Args:
            config: é…ç½®å­—å…¸
        """
        self.config = config
        self.logger = logging.getLogger(__name__)
        
        # Whisperé…ç½®
        self.whisper_config = config.get("whisper", {})
        self.backend = self.whisper_config.get("backend", "whisper")  # åç«¯é€‰æ‹©: "whisper" æˆ– "faster-whisper"
        self.model_size = self.whisper_config.get("model_size", "base")
        self.language = self.whisper_config.get("language", "auto")
        self.task = self.whisper_config.get("task", "transcribe")
        self.device = self.whisper_config.get("device", "auto")
        self.fp16 = self.whisper_config.get("fp16", False)  # FP16ç²¾åº¦åŠ é€Ÿé…ç½®
        
        # Faster-Whisper å‚æ•°é…ç½®
        self.faster_whisper_params = self.whisper_config.get("faster_whisper_params", {
            "beam_size": 5,
            "condition_on_previous_text": True,
            "compression_ratio_threshold": 2.4,
            "log_prob_threshold": -1.0,
            "no_speech_threshold": 0.6,
            "vad_filter": True,
            "vad_parameters": {
                "min_silence_duration_ms": 500
            }
        })
        
        # æ£€æŸ¥åç«¯å¯ç”¨æ€§
        if self.backend == "faster-whisper" and not FASTER_WHISPER_AVAILABLE:
            if WHISPER_AVAILABLE:
                self.logger.warning("Faster-Whisper ä¸å¯ç”¨ï¼Œå›é€€åˆ°åŸç”Ÿ Whisper")
                self.backend = "whisper"
            else:
                raise ImportError("é…ç½®ä¸º faster-whisperï¼Œä½† faster-whisper å’ŒåŸç”Ÿ whisper éƒ½ä¸å¯ç”¨")
        
        if self.backend == "whisper" and not WHISPER_AVAILABLE:
            if FASTER_WHISPER_AVAILABLE:
                self.logger.warning("åŸç”Ÿ Whisper ä¸å¯ç”¨ï¼Œå›é€€åˆ° Faster-Whisper")
                self.backend = "faster-whisper"
            else:
                raise ImportError("é…ç½®ä¸º whisperï¼Œä½† whisper å’Œ faster-whisper éƒ½ä¸å¯ç”¨")
        
        # åˆå§‹åŒ–æ¨¡å‹
        try:
            self.logger.info(f"ä½¿ç”¨åç«¯: {self.backend}")
            self.logger.info(f"åŠ è½½æ¨¡å‹: {self.model_size}")
            self.logger.info(f"FP16ç²¾åº¦åŠ é€Ÿ: {'å¯ç”¨' if self.fp16 else 'ç¦ç”¨'}")
            
            # è®°å½• Faster-Whisper å‚æ•°ï¼ˆå¦‚æœä½¿ç”¨ï¼‰
            if self.backend == "faster-whisper":
                self.logger.info(f"Faster-Whisper å‚æ•°: beam_size={self.faster_whisper_params.get('beam_size')}, "
                               f"vad_filter={self.faster_whisper_params.get('vad_filter')}, "
                               f"condition_on_previous_text={self.faster_whisper_params.get('condition_on_previous_text')}")
            
            if self.backend == "faster-whisper":
                self._init_faster_whisper()
            else:
                self._init_whisper()
                
            self.logger.info(f"{self.backend} æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            self.logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
        
        # åˆå§‹åŒ–åˆ†æ®µä¼˜åŒ–å™¨(æ ¹æ®é…ç½®é€‰æ‹©)
        self.segment_optimizer = None
        segmentation_config = self.whisper_config.get("segmentation", {})
        segmentation_method = segmentation_config.get("method", "semantic")  # é»˜è®¤ä½¿ç”¨ semantic

        if segmentation_method == "punctuation":
            try:
                self.segment_optimizer = PunctuationSegmentOptimizer(config)
                self.logger.info("åŸºäºæ ‡ç‚¹ç¬¦å·çš„åˆ†æ®µä¼˜åŒ–å™¨åˆå§‹åŒ–æˆåŠŸ")
            except Exception as e:
                self.logger.warning(f"æ ‡ç‚¹ç¬¦å·åˆ†æ®µä¼˜åŒ–å™¨åˆå§‹åŒ–å¤±è´¥: {e}ï¼Œå°†ä½¿ç”¨åŸå§‹åˆ†æ®µ")
                self.segment_optimizer = None
        elif segmentation_method != "semantic":
            self.logger.warning(f"ä¸æ”¯æŒçš„åˆ†æ®µæ–¹æ³•: {segmentation_method}ï¼Œå°†ä½¿ç”¨åŸå§‹åˆ†æ®µ")
        
        # åˆå§‹åŒ–è¯­ä¹‰åˆ†æ®µå™¨ï¼ˆæ€»æ˜¯å¯ç”¨ï¼‰
        try:
            self.semantic_segmenter = SemanticSegmenter(config)
            self.logger.info("è¯­ä¹‰åˆ†æ®µå™¨åˆå§‹åŒ–æˆåŠŸ")
        except Exception as e:
            self.logger.warning(f"è¯­ä¹‰åˆ†æ®µå™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            self.semantic_segmenter = None

    def _get_duration_seconds(self, audio_path: str) -> float:
        """è¿”å›éŸ³é¢‘æ—¶é•¿ï¼ˆç§’ï¼‰ï¼Œå¤±è´¥åˆ™è¿”å›0"""
        try:
            if sf is not None:
                f = sf.SoundFile(audio_path)
                return float(len(f)) / float(f.samplerate)
        except Exception:
            pass
        try:
            import librosa
            return float(librosa.get_duration(path=audio_path))
        except Exception:
            return 0.0

    def _should_use_punctuation_prompt(self, detected_language: str, duration_s: float) -> bool:
        """ä»…å¯¹è‹±æ–‡ä¸”è¾ƒé•¿å½•éŸ³å¯ç”¨æ ‡ç‚¹å¼•å¯¼ï¼Œé¿å…çŸ­å¥è¢«æ¨¡æ¿åç½®"""
        return (detected_language == "en") and (duration_s >= 8.0)
    
    def _detect_language_and_set_prompt(self, audio_path: str) -> Tuple[Optional[str], Optional[str]]:
        """
        æ£€æµ‹è¯­è¨€å¹¶è®¾ç½®æ ‡ç‚¹ç¬¦å·å¼•å¯¼
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            (detected_language, initial_prompt) å…ƒç»„
        """
        detected_language = self.language if self.language != "auto" else None
        initial_prompt = None
        duration_s = self._get_duration_seconds(audio_path)
        
        # å¦‚æœæ˜¯ä¸­æ–‡ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼•å¯¼æç¤ºè¯
        if detected_language == "zh":
            initial_prompt = "è¿™æ˜¯ä¸€æ®µä¸­æ–‡è¯­éŸ³è½¬å½•ï¼Œè¯·ä½¿ç”¨æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·ï¼ŒåŒ…æ‹¬å¥å·ã€é€—å·ã€é—®å·ç­‰ã€‚"
            self.logger.info("æ£€æµ‹åˆ°ä¸­æ–‡éŸ³é¢‘ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼•å¯¼æç¤ºè¯")
        elif detected_language == "en" and self._should_use_punctuation_prompt("en", duration_s):
            initial_prompt = "This is an English sentence with proper punctuation."
            self.logger.info("æ£€æµ‹åˆ°è‹±æ–‡é•¿è¯­éŸ³ï¼Œå¯ç”¨æ ‡ç‚¹å¼•å¯¼")
        elif detected_language is None and self.language == "auto":
            # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
            try:
                detection_result = self.detect_language(audio_path)
                detected_language = detection_result.get("detected_language", "en")
                if detected_language == "zh":
                    initial_prompt = "è¿™æ˜¯ä¸€æ®µä¸­æ–‡è¯­éŸ³è½¬å½•ï¼Œè¯·ä½¿ç”¨æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·ï¼ŒåŒ…æ‹¬å¥å·ã€é€—å·ã€é—®å·ç­‰ã€‚"
                    self.logger.info("è‡ªåŠ¨æ£€æµ‹åˆ°ä¸­æ–‡éŸ³é¢‘ï¼Œä½¿ç”¨æ ‡ç‚¹ç¬¦å·å¼•å¯¼æç¤ºè¯")
                elif detected_language == "en" and self._should_use_punctuation_prompt("en", duration_s):
                    initial_prompt = "This is an English sentence with proper punctuation."
                    self.logger.info("è‡ªåŠ¨æ£€æµ‹åˆ°è‹±æ–‡ä¸”ä¸ºé•¿è¯­éŸ³ï¼Œå¯ç”¨æ ‡ç‚¹å¼•å¯¼")
            except:
                detected_language = "en"
        
        return detected_language, initial_prompt
    
    def _init_whisper(self):
        """åˆå§‹åŒ–åŸç”Ÿ Whisper æ¨¡å‹"""
        if not WHISPER_AVAILABLE:
            raise ImportError("åŸç”Ÿ Whisper ä¸å¯ç”¨")
        
        import torch
        
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # å¼ºåˆ¶ä½¿ç”¨CUDAè®¾å¤‡ï¼Œè®¾ç½®ç¯å¢ƒå˜é‡é¿å…è®¾å¤‡è½¬æ¢é—®é¢˜
        if torch.cuda.is_available():
            self.device = "cuda"
            # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨CUDA
            os.environ["CUDA_VISIBLE_DEVICES"] = "0"
            torch.cuda.set_device(0)
            self.logger.info(f"ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
        
        # ä½¿ç”¨æ›´å®‰å…¨çš„æ¨¡å‹åŠ è½½æ–¹å¼
        try:
            # å…ˆå°è¯•åœ¨CPUä¸ŠåŠ è½½ï¼Œç„¶åç§»åŠ¨åˆ°CUDA
            if self.device == "cuda":
                try:
                    # æ–¹æ³•1ï¼šç›´æ¥åœ¨CUDAä¸ŠåŠ è½½
                    self.model = whisper.load_model(self.model_size, device=self.device)
                    self.logger.info(f"Whisperæ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {self.device}")
                except Exception as cuda_error:
                    # æ–¹æ³•2ï¼šå…ˆåœ¨CPUä¸ŠåŠ è½½ï¼Œç„¶åç§»åŠ¨åˆ°CUDA
                    self.logger.warning(f"ç›´æ¥CUDAåŠ è½½å¤±è´¥ï¼Œå°è¯•CPUåŠ è½½åç§»åŠ¨: {cuda_error}")
                    self.model = whisper.load_model(self.model_size, device="cpu")
                    
                    # å°†æ¨¡å‹ç§»åŠ¨åˆ°CUDAè®¾å¤‡
                    try:
                        self.model = self.model.to(self.device)
                        self.logger.info(f"Whisperæ¨¡å‹æˆåŠŸä»CPUç§»åŠ¨åˆ°è®¾å¤‡: {self.device}")
                    except Exception as move_error:
                        self.logger.warning(f"æ¨¡å‹ç§»åŠ¨åˆ°CUDAå¤±è´¥ï¼Œå›é€€åˆ°CPU: {move_error}")
                        self.device = "cpu"
            else:
                # CPUæ¨¡å¼
                self.model = whisper.load_model(self.model_size, device=self.device)
                self.logger.info(f"Whisperæ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {self.device}")
                
        except Exception as e:
            self.logger.error(f"Whisperæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise
    
    def _init_faster_whisper(self):
        """åˆå§‹åŒ– Faster-Whisper æ¨¡å‹"""
        import torch
        
        if self.device == "auto":
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        # Faster-Whisper è®¾å¤‡è®¾ç½®
        if self.device == "cuda" and torch.cuda.is_available():
            self.device = "cuda"
            # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ä½¿ç”¨CUDA
            os.environ["CUDA_VISIBLE_DEVICES"] = "0"
            torch.cuda.set_device(0)
            self.logger.info(f"ä½¿ç”¨CUDAè®¾å¤‡: {torch.cuda.get_device_name(0)}")
            
            # æ¸…ç†CUDAç¼“å­˜
            torch.cuda.empty_cache()
        else:
            self.device = "cpu"
            self.logger.info("ä½¿ç”¨CPUè®¾å¤‡")
        
        # è®¾ç½®è®¡ç®—ç±»å‹
        compute_type = "float16" if self.fp16 and self.device == "cuda" else "float32"
        
        # åŠ è½½ Faster-Whisper æ¨¡å‹
        self.model = FasterWhisperModel(
            self.model_size,
            device=self.device,
            compute_type=compute_type
        )
        self.logger.info(f"Faster-Whisperæ¨¡å‹æˆåŠŸåŠ è½½åˆ°è®¾å¤‡: {self.device}, è®¡ç®—ç±»å‹: {compute_type}")
    
    def _transcribe_faster_whisper(self, audio_path: str, language: Optional[str] = None, initial_prompt: Optional[str] = None) -> Dict[str, Any]:
        """ä½¿ç”¨ Faster-Whisper è¿›è¡Œè½¬å½•"""
        try:
            # ä»é…ç½®è¯»å– Faster-Whisper å‚æ•°
            params = self.faster_whisper_params.copy()
            
            # åŸºç¡€å‚æ•°ï¼ˆä½¿ç”¨ä¼ å…¥çš„ initial_promptï¼Œå¦‚æœè°ƒç”¨æ–¹è®¾ç½®äº†æ ‡ç‚¹ç¬¦å·å¼•å¯¼ï¼‰
            transcribe_params = {
                "language": language,
                "task": self.task,
                "word_timestamps": True,
                "initial_prompt": initial_prompt,  # ä½¿ç”¨ä¼ å…¥çš„å‚æ•°
            }
            
            # æ·»åŠ é…ç½®çš„ä¼˜åŒ–å‚æ•°
            transcribe_params.update({
                "beam_size": params.get("beam_size", 5),
                "condition_on_previous_text": False,
                "compression_ratio_threshold": params.get("compression_ratio_threshold", 2.4),
                "log_prob_threshold": params.get("log_prob_threshold", -1.0),
                "no_speech_threshold": params.get("no_speech_threshold", 0.6),
                "vad_filter": params.get("vad_filter", True),
            })

            # çŸ­éŸ³é¢‘å‡å° beam_sizeï¼Œé™ä½è¯­è¨€å…ˆéªŒå½±å“
            duration_s = self._get_duration_seconds(audio_path)
            if duration_s > 0 and duration_s <= 5.0:
                transcribe_params["beam_size"] = max(1, min(3, transcribe_params["beam_size"]))
            
            # æ·»åŠ  VAD å‚æ•°ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if transcribe_params["vad_filter"] and "vad_parameters" in params:
                transcribe_params["vad_parameters"] = params["vad_parameters"]
            
            self.logger.info(f"Faster-Whisper è½¬å½•å‚æ•°: beam_size={transcribe_params['beam_size']}, "
                           f"vad_filter={transcribe_params['vad_filter']}, "
                           f"condition_on_previous_text={transcribe_params['condition_on_previous_text']}, "
                           f"initial_prompt={'å·²è®¾ç½®' if initial_prompt else 'æœªè®¾ç½®'}")
            
            # æ‰§è¡Œè½¬å½•
            segments, info = self.model.transcribe(audio_path, **transcribe_params)
            
            # å°† segments è½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆä¿®å¤ generator é‡å¤æ¶ˆè€—é—®é¢˜ï¼‰
            segments_list = list(segments)
            self.logger.info(f"ğŸ” Faster-Whisper æ£€æµ‹åˆ° {len(segments_list)} ä¸ªåˆ†æ®µ")
            
            # è½¬æ¢ Faster-Whisper ç»“æœæ ¼å¼ä¸º Whisper å…¼å®¹æ ¼å¼
            result_text = ""
            result_segments = []
            
            for segment in segments_list:
                segment_text = segment.text.strip()
                result_text += segment_text + " "
                
                # è½¬æ¢å•è¯æ—¶é—´æˆ³
                words = []
                if hasattr(segment, 'words') and segment.words:
                    for word in segment.words:
                        words.append({
                            "word": word.word,
                            "start": word.start,
                            "end": word.end,
                            "probability": getattr(word, 'probability', 1.0)
                        })
                
                result_segments.append({
                    "id": len(result_segments),
                    "seek": 0,  # Faster-Whisper ä¸æä¾› seek
                    "start": segment.start,
                    "end": segment.end,
                    "text": segment_text,
                    "tokens": [],  # Faster-Whisper ä¸æä¾› tokens
                    "temperature": 0.0,
                    "avg_logprob": getattr(segment, 'avg_logprob', 0.0),
                    "compression_ratio": getattr(segment, 'compression_ratio', 1.0),
                    "no_speech_prob": getattr(segment, 'no_speech_prob', 0.0),
                    "words": words
                })
            
            # æ„å»ºç»“æœå­—å…¸
            result = {
                "text": result_text.strip(),
                "language": info.language if hasattr(info, 'language') else language or "auto",
                "language_probability": getattr(info, 'language_probability', 1.0),
                "duration": getattr(info, 'duration', 0.0),
                "duration_after_vad": getattr(info, 'duration_after_vad', 0.0),
                "all_language_probs": getattr(info, 'all_language_probs', {}),
                "segments": result_segments
            }
            
            return result
            
        except Exception as e:
            self.logger.error(f"Faster-Whisper è½¬å½•å¤±è´¥: {e}")
            raise
    
    def transcribe_audio(self, audio_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        è½¬å½•éŸ³é¢‘æ–‡ä»¶
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        if not validate_file_path(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        self.logger.info(f"å¼€å§‹è¯­éŸ³è¯†åˆ«: {audio_path}")
        
        try:
            # æ£€æµ‹è¯­è¨€å¹¶è®¾ç½®æ ‡ç‚¹ç¬¦å·å¼•å¯¼
            detected_language, initial_prompt = self._detect_language_and_set_prompt(audio_path)
            
            # æ‰§è¡Œè½¬å½•
            if self.backend == "faster-whisper":
                result = self._transcribe_faster_whisper(audio_path, detected_language, initial_prompt)
            else:
                result = self.model.transcribe(
                audio_path,
                language=detected_language,
                task=self.task,
                verbose=False,
                word_timestamps=True,  # å¯ç”¨å•è¯æ—¶é—´æˆ³ï¼Œè·å¾—æ›´ç²¾ç¡®çš„åˆ†æ®µ
                initial_prompt=initial_prompt,  # ä½¿ç”¨è®¾ç½®çš„æ ‡ç‚¹ç¬¦å·å¼•å¯¼
                # ä¼˜åŒ–çš„åˆ†æ®µå‚æ•°
                condition_on_previous_text=False,  # å…³é—­è·¨æ®µä¸Šä¸‹æ–‡
                compression_ratio_threshold=1.2,  # é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´è‡ªç„¶çš„åˆ†æ®µ
                no_speech_threshold=0.2,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
            )
            
            # å¤„ç†è½¬å½•ç»“æœ
            transcription_result = self._process_transcription_result(result, audio_path, output_dir)
            
            self.logger.info("è¯­éŸ³è¯†åˆ«å®Œæˆ")
            return transcription_result
            
        except Exception as e:
            self.logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            raise
    
    def transcribe_with_output_manager(self, audio_path: str, output_manager: OutputManager) -> Dict[str, Any]:
        """
        ä½¿ç”¨OutputManagerè¿›è¡Œè¯­éŸ³è¯†åˆ«
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_manager: è¾“å‡ºç®¡ç†å™¨å®ä¾‹
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        if not validate_file_path(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        self.logger.info(f"å¼€å§‹è¯­éŸ³è¯†åˆ«: {audio_path}")
        output_manager.log(f"æ­¥éª¤4å¼€å§‹: è¯­éŸ³è¯†åˆ« {audio_path}")
        
        try:
            # æ£€æµ‹è¯­è¨€å¹¶è®¾ç½®æ ‡ç‚¹ç¬¦å·å¼•å¯¼
            detected_language, initial_prompt = self._detect_language_and_set_prompt(audio_path)
            
            # æ‰§è¡Œè½¬å½•
            if self.backend == "faster-whisper":
                result = self._transcribe_faster_whisper(audio_path, detected_language, initial_prompt)
            else:
                result = self.model.transcribe(
                audio_path,
                language=detected_language,
                task=self.task,
                verbose=False,
                word_timestamps=True,  # å¯ç”¨å•è¯æ—¶é—´æˆ³ï¼Œè·å¾—æ›´ç²¾ç¡®çš„åˆ†æ®µ
                initial_prompt=initial_prompt,  # ä½¿ç”¨è®¾ç½®çš„æ ‡ç‚¹ç¬¦å·å¼•å¯¼
                # ä¼˜åŒ–çš„åˆ†æ®µå‚æ•°
                condition_on_previous_text=False,
                compression_ratio_threshold=1.2,  # é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´è‡ªç„¶çš„åˆ†æ®µ
                no_speech_threshold=0.2,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
            )
            
            # æ·»åŠ  Whisper åŸå§‹åˆ†æ®µç»Ÿè®¡æ—¥å¿—
            self.logger.info(f"ğŸ” Whisper åŸå§‹åˆ†æ®µæ•°: {len(result.get('segments', []))}")
            for i, seg in enumerate(result.get("segments", [])):
                self.logger.debug(f"  åˆ†æ®µ {i+1}: {seg.get('start', 0):.2f}s - {seg.get('end', 0):.2f}s, "
                                 f"æ–‡æœ¬: '{seg.get('text', '')[:50]}...', "
                                 f"å•è¯æ•°: {len(seg.get('words', []))}")
            
            # æ ¹æ®é…ç½®é€‰æ‹©åˆ†æ®µä¼˜åŒ–æ–¹å¼
            segmentation_config = self.whisper_config.get("segmentation", {})
            segmentation_method = segmentation_config.get("method", "semantic")

            if segmentation_method == "punctuation" and self.segment_optimizer is not None:
                # ä½¿ç”¨ PunctuationSegmentOptimizer (éœ€è¦å…ˆä¿å­˜ä¸´æ—¶æ–‡ä»¶)
                # 1. å…ˆè°ƒç”¨ _process_transcription_result_with_output_manager ä¿å­˜åŸºç¡€æ–‡ä»¶
                transcription_result = self._process_transcription_result_with_output_manager(
                    result, audio_path, output_manager
                )
                
                # 2. è·å–å·²ä¿å­˜çš„æ–‡ä»¶è·¯å¾„
                transcription_file = output_manager.get_file_path(StepNumbers.STEP_4, "whisper_raw_transcription")
                
                # 3. è°ƒç”¨ PunctuationSegmentOptimizer.optimize_segments
                try:
                    # ä»åŸå§‹ Whisper ç»“æœä¸­æå–å•è¯çº§æ—¶é—´æˆ³
                    raw_word_timestamps = []
                    for segment in result["segments"]:
                        if "words" in segment:
                            raw_word_timestamps.extend(segment["words"])
                    
                    optimized_segments = self.segment_optimizer.optimize_segments(
                        transcription_file, 
                        raw_word_timestamps
                    )
                    
                    # 4. æ›´æ–° transcription_result ä¸­çš„ segments
                    transcription_result["segments"] = optimized_segments
                    
                    # 5. é‡æ–°ä¿å­˜ä¼˜åŒ–åçš„ segments æ–‡ä»¶
                    segments_file = output_manager.get_file_path(StepNumbers.STEP_4, "segments_txt")
                    segments_json_file = output_manager.get_file_path(StepNumbers.STEP_4, "segments_json")
                    
                    # ä¿å­˜æ›´æ–°åçš„ segments
                    self._save_optimized_segments(optimized_segments, segments_file, segments_json_file)
                    
                    self.logger.info(f"åˆ†æ®µä¼˜åŒ–å®Œæˆ(punctuation): {len(result['segments'])} -> {len(optimized_segments)} ä¸ªç‰‡æ®µ")
                except Exception as e:
                    self.logger.warning(f"æ ‡ç‚¹ç¬¦å·åˆ†æ®µä¼˜åŒ–å¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹åˆ†æ®µ")
            elif segmentation_method == "semantic" and self.semantic_segmenter is not None:
                # ä½¿ç”¨æ–°çš„è¯­ä¹‰åˆ†æ®µå™¨
                self.logger.info("ä½¿ç”¨è¯­ä¹‰åˆ†æ®µå™¨è¿›è¡Œæ™ºèƒ½åˆ†æ®µ")
                
                # 1. æ”¶é›†æ‰€æœ‰å•è¯æ—¶é—´æˆ³
                all_words = []
                for seg in result.get("segments", []):
                    all_words.extend(seg.get("words", []))
                
                # 2. ä½¿ç”¨è¯­ä¹‰åˆ†æ®µå™¨é‡æ–°åˆ†æ®µ
                try:
                    semantic_segments = self.semantic_segmenter.segment(all_words, result.get("text", ""))
                    
                    # 3. æ›´æ–° result ä¸­çš„ segments
                    result["segments"] = semantic_segments
                    
                    # 4. å¤„ç†è½¬å½•ç»“æœ
                    transcription_result = self._process_transcription_result_with_output_manager(
                        result, audio_path, output_manager, apply_optimization=False
                    )
                    
                    self.logger.info(f"è¯­ä¹‰åˆ†æ®µå®Œæˆ: {len(all_words)} ä¸ªå•è¯ -> {len(semantic_segments)} ä¸ªåˆ†æ®µ")
                except Exception as e:
                    self.logger.warning(f"è¯­ä¹‰åˆ†æ®µå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹åˆ†æ®µ")
                    transcription_result = self._process_transcription_result_with_output_manager(
                        result, audio_path, output_manager, apply_optimization=False
                    )
            else:
                # ä½¿ç”¨åŸå§‹åˆ†æ®µï¼ˆä¸æ”¯æŒçš„åˆ†æ®µæ–¹æ³•æˆ–åˆ†æ®µå™¨ä¸å¯ç”¨ï¼‰
                self.logger.info("ä½¿ç”¨ Whisper åŸå§‹åˆ†æ®µï¼ˆæ— åå¤„ç†ï¼‰")
                transcription_result = self._process_transcription_result_with_output_manager(
                    result, audio_path, output_manager, apply_optimization=False
                )
            
            self.logger.info("è¯­éŸ³è¯†åˆ«å®Œæˆ")
            output_manager.log("æ­¥éª¤4å®Œæˆ: è¯­éŸ³è¯†åˆ«å®Œæˆ")
            return transcription_result
            
        except Exception as e:
            self.logger.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            output_manager.log(f"æ­¥éª¤4å¤±è´¥: {e}")
            raise
    
    def transcribe_with_segments(self, audio_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        å¸¦æ—¶é—´æ®µçš„è¯¦ç»†è½¬å½•
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            è¯¦ç»†è½¬å½•ç»“æœå­—å…¸
        """
        if not validate_file_path(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        self.logger.info(f"å¼€å§‹è¯¦ç»†è¯­éŸ³è¯†åˆ«: {audio_path}")
        
        try:
            # æ£€æµ‹è¯­è¨€å¹¶è®¾ç½®æ ‡ç‚¹ç¬¦å·å¼•å¯¼
            detected_language, initial_prompt = self._detect_language_and_set_prompt(audio_path)
            
            # æ‰§è¡Œè¯¦ç»†è½¬å½•ï¼Œé‡‡ç”¨ä¼˜åŒ–çš„åˆ†æ®µç­–ç•¥
            if self.backend == "faster-whisper":
                result = self._transcribe_faster_whisper(audio_path, detected_language, initial_prompt)
            else:
                result = self.model.transcribe(
                audio_path,
                language=detected_language,
                task=self.task,
                verbose=False,
                word_timestamps=True,  # å¯ç”¨å•è¯æ—¶é—´æˆ³ï¼Œè·å¾—æ›´ç²¾ç¡®çš„åˆ†æ®µ
                initial_prompt=initial_prompt,  # æ·»åŠ æ ‡ç‚¹ç¬¦å·å¼•å¯¼
                # ä¼˜åŒ–çš„åˆ†æ®µå‚æ•°
                condition_on_previous_text=True,  # è€ƒè™‘ä¸Šä¸‹æ–‡è¿è´¯æ€§
                compression_ratio_threshold=1.2,  # é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´è‡ªç„¶çš„åˆ†æ®µ
                no_speech_threshold=0.2,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
                best_of=1,  # åªç”Ÿæˆä¸€ä¸ªç»“æœ
                beam_size=5,  # ä½¿ç”¨beam search
                temperature=0.0,  # ç¡®å®šæ€§è¾“å‡º
                patience=1.0,  # æ ‡å‡†è€å¿ƒå‚æ•°
                    fp16=self.fp16  # ğŸš€ ä½¿ç”¨é…ç½®ä¸­çš„FP16è®¾ç½®
            )
            
            # ä¿å­˜WhisperåŸå§‹å•è¯æ—¶é—´æˆ³
            if output_dir:
                self._save_whisper_word_timestamps(result, audio_path, output_dir)
            
            # å¤„ç†è¯¦ç»†è½¬å½•ç»“æœ
            transcription_result = self._process_detailed_transcription_result(result, audio_path, output_dir)
            
            self.logger.info("è¯¦ç»†è¯­éŸ³è¯†åˆ«å®Œæˆ")
            return transcription_result
            
        except Exception as e:
            self.logger.error(f"è¯¦ç»†è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            raise
    
    def transcribe_with_translation(self, audio_path: str, output_dir: Optional[str] = None, config: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """
        è¯­éŸ³è¯†åˆ«ã€åˆ†æ®µä¼˜åŒ–å’Œç¿»è¯‘çš„å®Œæ•´æµç¨‹
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            config: é…ç½®å­—å…¸ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            åŒ…å«è½¬å½•ã€åˆ†æ®µå’Œç¿»è¯‘ç»“æœçš„å­—å…¸
        """
        if not validate_file_path(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        self.logger.info(f"å¼€å§‹è¯­éŸ³è¯†åˆ«å’Œåˆ†æ®µä¼˜åŒ–: {audio_path}")
        
        try:
            # æ£€æµ‹è¯­è¨€å¹¶è®¾ç½®æ ‡ç‚¹ç¬¦å·å¼•å¯¼
            detected_language, initial_prompt = self._detect_language_and_set_prompt(audio_path)
            
            # é¦–å…ˆæ‰§è¡Œè½¬å½• - ä¼˜åŒ–åˆ†æ®µå‚æ•°
            if self.backend == "faster-whisper":
                result = self._transcribe_faster_whisper(audio_path, detected_language, initial_prompt)
            else:
                result = self.model.transcribe(
                audio_path,
                language=detected_language,
                task=self.task,
                verbose=False,
                word_timestamps=True,  # å¯ç”¨å•è¯æ—¶é—´æˆ³ï¼Œè·å¾—æ›´ç²¾ç¡®çš„åˆ†æ®µ
                initial_prompt=initial_prompt,  # ä½¿ç”¨è®¾ç½®çš„æ ‡ç‚¹ç¬¦å·å¼•å¯¼
                # ä¼˜åŒ–çš„åˆ†æ®µå‚æ•°
                condition_on_previous_text=False,
                compression_ratio_threshold=1.2,  # é™ä½é˜ˆå€¼ï¼Œå…è®¸æ›´è‡ªç„¶çš„åˆ†æ®µ
                no_speech_threshold=0.2,  # é™ä½é˜ˆå€¼ï¼Œä¿ç•™æ›´å¤šå†…å®¹
                best_of=1,
                beam_size=3,
                temperature=0.0,
                patience=1.0,
                    fp16=self.fp16  # ğŸš€ ä½¿ç”¨é…ç½®ä¸­çš„FP16è®¾ç½®
            )
            
            # ä¿å­˜WhisperåŸå§‹å•è¯æ—¶é—´æˆ³
            if output_dir:
                self._save_whisper_word_timestamps(result, audio_path, output_dir)
            
            # æå–åŸºç¡€åˆ†æ®µä¿¡æ¯
            segments = result.get("segments", [])
            whisper_segments = []
            for segment in segments:
                whisper_segments.append({
                    "start": segment.get("start", 0.0),
                    "end": segment.get("end", 0.0),
                    "text": segment.get("text", "").strip(),
                    "words": segment.get("words", [])
                })
            
            # ä½¿ç”¨åˆ†æ®µä¼˜åŒ–å™¨ï¼ˆåªæ”¯æŒpunctuationå’Œsemanticï¼‰
            segmentation_config = self.whisper_config.get("segmentation", {})
            segmentation_method = segmentation_config.get("method", "semantic")
            
            if segmentation_method == "punctuation" and self.segment_optimizer:
                self.logger.info("å¼€å§‹åŸºäºæ ‡ç‚¹ç¬¦å·çš„åˆ†æ®µä¼˜åŒ–...")
                if output_dir:
                    create_output_dir(output_dir)
                    input_name = Path(audio_path).stem
                    safe_name = safe_filename(input_name)
                    transcription_file = os.path.join(output_dir, f"{safe_name}_transcription.txt")
                    
                    # å…ˆåˆ›å»ºè½¬å½•æ–‡ä»¶
                    text = " ".join([segment.get("text", "").strip() for segment in whisper_segments])
                    with open(transcription_file, 'w', encoding='utf-8') as f:
                        f.write(text)
                    self.logger.info(f"è½¬å½•æ–‡ä»¶å·²åˆ›å»º: {transcription_file}")
                    
                    # è·å–å•è¯æ—¶é—´æˆ³
                    word_timestamps = []
                    for segment in whisper_segments:
                        if "words" in segment:
                            word_timestamps.extend(segment["words"])
                    
                    optimized_segments = self.segment_optimizer.optimize_segments(
                        transcription_file, word_timestamps
                    )
                    
                    # ä¿å­˜ä¼˜åŒ–ç»“æœ
                    optimization_file = os.path.join(output_dir, f"{safe_name}_punctuation_segments.json")
                    self.segment_optimizer.save_optimization_result(optimized_segments, optimization_file)
                else:
                    self.logger.warning("è¾“å‡ºç›®å½•æœªæŒ‡å®šï¼Œæ— æ³•è¿›è¡Œæ ‡ç‚¹ç¬¦å·åˆ†æ®µï¼Œä½¿ç”¨åŸå§‹åˆ†æ®µ")
                    optimized_segments = whisper_segments
            elif segmentation_method == "semantic" and self.semantic_segmenter:
                self.logger.info("å¼€å§‹è¯­ä¹‰åˆ†æ®µä¼˜åŒ–...")
                # æ”¶é›†æ‰€æœ‰å•è¯æ—¶é—´æˆ³
                all_words = []
                for seg in whisper_segments:
                    all_words.extend(seg.get("words", []))
                
                # ä½¿ç”¨è¯­ä¹‰åˆ†æ®µå™¨é‡æ–°åˆ†æ®µ
                try:
                    full_text = " ".join([seg.get("text", "").strip() for seg in whisper_segments])
                    optimized_segments = self.semantic_segmenter.segment(all_words, full_text)
                    self.logger.info(f"è¯­ä¹‰åˆ†æ®µå®Œæˆ: {len(all_words)} ä¸ªå•è¯ -> {len(optimized_segments)} ä¸ªåˆ†æ®µ")
                except Exception as e:
                    self.logger.warning(f"è¯­ä¹‰åˆ†æ®µå¤±è´¥: {e}ï¼Œä½¿ç”¨åŸå§‹åˆ†æ®µ")
                    optimized_segments = whisper_segments
            else:
                self.logger.info("ä½¿ç”¨ Whisper åŸå§‹åˆ†æ®µï¼ˆæ— åˆ†æ®µä¼˜åŒ–ï¼‰")
                optimized_segments = whisper_segments
            
            # æ¸…ç†å†…å­˜
            import gc
            gc.collect()
            
            # ä½¿ç”¨ç‹¬ç«‹çš„ç¿»è¯‘æ¨¡å—è¿›è¡Œç¿»è¯‘
            self.logger.info("å¼€å§‹ç‹¬ç«‹ç¿»è¯‘...")
            from .text_translator import TextTranslator
            
            # ä½¿ç”¨ä¼ å…¥çš„é…ç½®ï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
            translation_config = config if config is not None else self.config
            translator = TextTranslator(translation_config)
            
            # å‡†å¤‡ç¿»è¯‘æ•°æ®
            segments_for_translation = []
            for segment in optimized_segments:
                segments_for_translation.append({
                    "start": segment.get("start", 0.0),
                    "end": segment.get("end", 0.0),
                    "text": segment.get("text", "")
                })
            
            # æ‰§è¡Œç¿»è¯‘
            translation_result = translator.translate_segments(segments_for_translation, output_dir)
            
            if translation_result["success"]:
                translated_segments = translation_result.get("translated_segments", [])
                translation_info = translation_result.get("translation_info", {})
                
                # æ£€æŸ¥æ˜¯å¦è·³è¿‡äº†ç¿»è¯‘
                if translation_info.get("method") == "skip_translation":
                    self.logger.info(f"ğŸš€ ç¿»è¯‘ä¼˜åŒ–ç”Ÿæ•ˆ: {translation_info.get('reason', '')}")
                
                # åˆå¹¶ä¼˜åŒ–å’Œç¿»è¯‘ç»“æœ
                final_segments = []
                for i, (optimized, translated) in enumerate(zip(optimized_segments, translated_segments)):
                    final_segments.append({
                        **optimized,
                        "translated_text": translated.get("translated_text", ""),
                        "translation_info": translated.get("translation_info", {})
                    })
                optimized_segments = final_segments
                self.logger.info(f"âœ… ç‹¬ç«‹ç¿»è¯‘å®Œæˆï¼Œå¤„ç†äº† {len(translated_segments)} ä¸ªåˆ†æ®µ")
            else:
                self.logger.warning("ç¿»è¯‘å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ä¼˜åŒ–åˆ†æ®µ")
            
            # ä¿å­˜æ‰€æœ‰å¤„ç†ç»“æœ
            transcription_result = self._save_processing_results(
                result, optimized_segments, audio_path, output_dir
            )
            
            self.logger.info("è¯­éŸ³è¯†åˆ«ã€åˆ†æ®µä¼˜åŒ–å’Œç¿»è¯‘å®Œæˆ")
            return transcription_result
            
        except Exception as e:
            self.logger.error(f"è¯­éŸ³è¯†åˆ«ã€åˆ†æ®µä¼˜åŒ–å’Œç¿»è¯‘å¤±è´¥: {e}")
            raise
    
    def detect_language(self, audio_path: str) -> Dict[str, Any]:
        """
        æ£€æµ‹éŸ³é¢‘è¯­è¨€
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            è¯­è¨€æ£€æµ‹ç»“æœ
        """
        if not validate_file_path(audio_path):
            raise FileNotFoundError(f"éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
        
        self.logger.info(f"å¼€å§‹è¯­è¨€æ£€æµ‹: {audio_path}")
        
        try:
            if self.backend == "faster-whisper":
                # ä½¿ç”¨ Faster-Whisper è¿›è¡Œè¯­è¨€æ£€æµ‹
                segments, info = self.model.transcribe(audio_path, language=None, task="transcribe", beam_size=1, best_of=1, temperature=0.0, patience=1.0, length_penalty=1.0, repetition_penalty=1.0, no_repeat_ngram_size=0, initial_prompt=None, prefix=None, suppress_blank=True, suppress_tokens=[-1], without_timestamps=True, max_initial_timestamp=0.0, word_timestamps=False, vad_filter=False)
                
                detected_language = info.language if hasattr(info, 'language') else "auto"
                confidence = getattr(info, 'language_probability', 1.0)
                all_probs = getattr(info, 'all_language_probs', {})
                
                result = {
                    "detected_language": detected_language,
                    "confidence": confidence,
                    "all_probabilities": all_probs,
                    "audio_path": audio_path
                }
            else:
                # ä½¿ç”¨åŸç”Ÿ Whisper è¿›è¡Œè¯­è¨€æ£€æµ‹
                if not WHISPER_AVAILABLE:
                    raise ImportError("åŸç”Ÿ Whisper ä¸å¯ç”¨")
                
                audio = whisper.load_audio(audio_path)
                audio = whisper.pad_or_trim(audio)
                
                # æ£€æµ‹è¯­è¨€
                mel = whisper.log_mel_spectrogram(audio).to(self.model.device)
                _, probs = self.model.detect_language(mel)
                
                # è·å–æœ€å¯èƒ½çš„è¯­è¨€
                detected_language = max(probs, key=probs.get)
                confidence = probs[detected_language]
                
                result = {
                    "detected_language": detected_language,
                    "confidence": confidence,
                    "all_probabilities": probs,
                    "audio_path": audio_path
                }
            
            self.logger.info(f"è¯­è¨€æ£€æµ‹å®Œæˆ: {detected_language} (ç½®ä¿¡åº¦: {confidence:.3f})")
            return result
            
        except Exception as e:
            self.logger.error(f"è¯­è¨€æ£€æµ‹å¤±è´¥: {e}")
            raise
    
    def _process_transcription_result(self, result: Dict[str, Any], audio_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†è½¬å½•ç»“æœ
        
        Args:
            result: WhisperåŸå§‹ç»“æœ
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†åçš„è½¬å½•ç»“æœ
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        text = result.get("text", "").strip()
        language = result.get("language", "unknown")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir:
            create_output_dir(output_dir)
            
            # ä¿å­˜è½¬å½•æ–‡æœ¬
            input_name = Path(audio_path).stem
            safe_name = safe_filename(input_name)
            text_file = os.path.join(output_dir, f"{safe_name}_transcription.txt")
            
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(text)
        else:
            text_file = None
        
        return {
            "success": True,
            "text": text,
            "language": language,
            "audio_path": audio_path,
            "text_file": text_file,
            "processing_info": {
                "model_size": self.model_size,
                "task": self.task,
                "language_detected": language
            }
        }
    
    def _process_transcription_result_with_output_manager(self, result: Dict[str, Any], audio_path: str, output_manager: OutputManager, apply_optimization: bool = False) -> Dict[str, Any]:
        """
        ä½¿ç”¨OutputManagerå¤„ç†è½¬å½•ç»“æœ
        
        Args:
            result: WhisperåŸå§‹ç»“æœ
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_manager: è¾“å‡ºç®¡ç†å™¨å®ä¾‹
            apply_optimization: æ˜¯å¦åº”ç”¨åˆ†æ®µä¼˜åŒ–ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ä»…ä¸ºå…¼å®¹æ€§ï¼‰
            
        Returns:
            å¤„ç†åçš„è½¬å½•ç»“æœ
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        text = result.get("text", "").strip()
        language = result.get("language", "unknown")
        segments = result.get("segments", [])
        
        # å¤„ç†æ—¶é—´æ®µä¿¡æ¯
        processed_segments = []
        for segment in segments:
            processed_segments.append({
                "start": segment.get("start", 0.0),
                "end": segment.get("end", 0.0),
                "text": segment.get("text", "").strip(),
                "words": segment.get("words", []),
                "speaker_id": segment.get("speaker_id")  # ä¿ç•™speaker_idï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            })
        
        # ä¸å†ä½¿ç”¨å†…ç½®çš„_optimize_segmentsï¼Œåˆ†æ®µä¼˜åŒ–ç”±å¤–éƒ¨æ–¹æ³•ï¼ˆpunctuation/semanticï¼‰å¤„ç†
        self.logger.info(f"å¤„ç†è½¬å½•ç»“æœ: {len(processed_segments)} ä¸ªåˆ†æ®µ")
        
        # ä½¿ç”¨OutputManagerç”Ÿæˆæ–‡ä»¶è·¯å¾„
        transcription_file = output_manager.get_file_path(StepNumbers.STEP_4, "whisper_raw_transcription")
        word_timestamps_file = output_manager.get_file_path(StepNumbers.STEP_4, "whisper_raw_word_timestamps")
        segments_txt_file = output_manager.get_file_path(StepNumbers.STEP_4, "segments_txt")
        segments_json_file = output_manager.get_file_path(StepNumbers.STEP_4, "segments_json")
        
        # ä¿å­˜ Whisper åŸå§‹è¾“å‡ºï¼ˆç”¨äºè°ƒè¯•ï¼‰
        raw_result_file = output_manager.get_file_path(StepNumbers.STEP_4, "whisper_raw")
        with open(raw_result_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        # ä¿å­˜ Whisper åŸå§‹åˆ†æ®µï¼ˆå¯è¯»æ ¼å¼ï¼‰
        raw_segments_file = output_manager.get_file_path(StepNumbers.STEP_4, "whisper_raw_segments")
        with open(raw_segments_file, 'w', encoding='utf-8') as f:
            f.write("Whisper åŸå§‹åˆ†æ®µ:\n")
            f.write("=" * 60 + "\n\n")
            for i, seg in enumerate(result.get("segments", [])):
                f.write(f"åˆ†æ®µ {i+1}:\n")
                f.write(f"  æ—¶é—´: {seg.get('start', 0):.3f}s - {seg.get('end', 0):.3f}s\n")
                f.write(f"  æ–‡æœ¬: {seg.get('text', '')}\n")
                f.write(f"  å•è¯æ•°: {len(seg.get('words', []))}\n\n")
        
        # ä¿å­˜è½¬å½•æ–‡æœ¬
        with open(transcription_file, 'w', encoding='utf-8') as f:
            f.write(text)
        
        # ä¿å­˜å•è¯çº§æ—¶é—´æˆ³ï¼ˆä»åŸå§‹ result æå–ï¼Œç¡®ä¿å®Œæ•´ï¼‰
        word_timestamps = []
        for segment in result.get("segments", []):  # æ”¹ç”¨ result è€Œä¸æ˜¯ processed_segments
            for word in segment.get("words", []):
                word_timestamps.append({
                    "word": word.get("word", ""),
                    "start": word.get("start", 0.0),
                    "end": word.get("end", 0.0),
                    "probability": word.get("probability", 0.0)
                })
        
        with open(word_timestamps_file, 'w', encoding='utf-8') as f:
            f.write("Whisper åŸå§‹å•è¯æ—¶é—´æˆ³ï¼ˆå®Œæ•´æ•°æ®ï¼‰:\n")
            f.write("=" * 60 + "\n\n")
            for word_info in word_timestamps:
                f.write(f"{word_info['start']:.3f} - {word_info['end']:.3f}: {word_info['word']} (prob: {word_info['probability']:.3f})\n")
        
        # ä¿å­˜åˆ†æ®µæ–‡æœ¬
        with open(segments_txt_file, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(processed_segments):
                f.write(f"Segment {i+1} ({segment['start']:.3f}s - {segment['end']:.3f}s):\n")
                f.write(f"{segment['text']}\n\n")
        
        # ä¿å­˜åˆ†æ®µJSONæ•°æ®
        with open(segments_json_file, 'w', encoding='utf-8') as f:
            json.dump(processed_segments, f, ensure_ascii=False, indent=2)
        
        # è®°å½•æ—¥å¿—
        self.logger.info(f"ğŸ“Š å•è¯æ—¶é—´æˆ³ç»Ÿè®¡: æ€»è®¡ {len(word_timestamps)} ä¸ªå•è¯")
        if word_timestamps:
            self.logger.info(f"  æ—¶é—´èŒƒå›´: {word_timestamps[0]['start']:.2f}s - {word_timestamps[-1]['end']:.2f}s")
        
        output_manager.log(f"è½¬å½•æ–‡ä»¶å·²ä¿å­˜:")
        output_manager.log(f"  - è½¬å½•æ–‡æœ¬: {transcription_file}")
        output_manager.log(f"  - å•è¯æ—¶é—´æˆ³: {word_timestamps_file}")
        output_manager.log(f"  - åˆ†æ®µæ–‡æœ¬: {segments_txt_file}")
        output_manager.log(f"  - åˆ†æ®µJSON: {segments_json_file}")
        output_manager.log(f"  - WhisperåŸå§‹è¾“å‡º: {raw_result_file}")
        output_manager.log(f"  - WhisperåŸå§‹åˆ†æ®µ: {raw_segments_file}")
        
        return {
            "success": True,
            "text": text,
            "language": language,
            "audio_path": audio_path,
            "transcription_file": transcription_file,
            "word_timestamps_file": word_timestamps_file,
            "segments_txt_file": segments_txt_file,
            "segments_json_file": segments_json_file,
            "segments": processed_segments,
            "processing_info": {
                "model_size": self.model_size,
                "task": self.task,
                "language_detected": language,
                "segments_count": len(processed_segments)
            }
        }
    
    def _process_detailed_transcription_result(self, result: Dict[str, Any], audio_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        å¤„ç†è¯¦ç»†è½¬å½•ç»“æœ
        
        Args:
            result: WhisperåŸå§‹ç»“æœ
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            å¤„ç†åçš„è¯¦ç»†è½¬å½•ç»“æœ
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        text = result.get("text", "").strip()
        language = result.get("language", "unknown")
        segments = result.get("segments", [])
        
        # å¤„ç†æ—¶é—´æ®µä¿¡æ¯ï¼Œå¹¶è¿›è¡Œåå¤„ç†ä¼˜åŒ–
        processed_segments = []
        for segment in segments:
            processed_segments.append({
                "start": segment.get("start", 0.0),
                "end": segment.get("end", 0.0),
                "text": segment.get("text", "").strip(),
                "words": segment.get("words", [])
            })
        
        # ä¸å†ä½¿ç”¨å†…ç½®çš„_optimize_segmentsï¼Œåˆ†æ®µä¼˜åŒ–ç”±å¤–éƒ¨æ–¹æ³•ï¼ˆpunctuation/semanticï¼‰å¤„ç†
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir:
            create_output_dir(output_dir)
            
            # ä¿å­˜è¯¦ç»†è½¬å½•ç»“æœ
            input_name = Path(audio_path).stem
            safe_name = safe_filename(input_name)
            
            # ä¿å­˜æ–‡æœ¬æ–‡ä»¶
            text_file = os.path.join(output_dir, f"{safe_name}_transcription.txt")
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(text)
            
        else:
            text_file = None
        
        return {
            "success": True,
            "text": text,
            "language": language,
            "segments": processed_segments,
            "audio_path": audio_path,
            "text_file": text_file,
            "processing_info": {
                "model_size": self.model_size,
                "task": self.task,
                "language_detected": language,
                "segment_count": len(processed_segments)
            }
        }
    
    def _save_processing_results(self, whisper_result: Dict[str, Any], 
                               optimized_segments: List[Dict[str, Any]], 
                               audio_path: str, output_dir: Optional[str] = None) -> Dict[str, Any]:
        """
        ä¿å­˜æ‰€æœ‰å¤„ç†ç»“æœåˆ°æ–‡ä»¶å¹¶è¿”å›æ±‡æ€»ä¿¡æ¯
        
        Args:
            whisper_result: WhisperåŸå§‹ç»“æœ
            optimized_segments: ä¼˜åŒ–åçš„åˆ†æ®µï¼ˆå¯èƒ½åŒ…å«ç¿»è¯‘ç»“æœï¼‰
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            
        Returns:
            åŒ…å«æ‰€æœ‰æ–‡ä»¶è·¯å¾„å’Œå¤„ç†ä¿¡æ¯çš„å­—å…¸
        """
        # æå–åŸºæœ¬ä¿¡æ¯
        text = whisper_result.get("text", "").strip()
        language = whisper_result.get("language", "unknown")
        whisper_segments = whisper_result.get("segments", [])
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        if output_dir:
            create_output_dir(output_dir)
            
            # ä¿å­˜è½¬å½•æ–‡æœ¬
            input_name = Path(audio_path).stem
            safe_name = safe_filename(input_name)
            
            # ä¿å­˜å®Œæ•´æ–‡æœ¬
            text_file = os.path.join(output_dir, f"{safe_name}_transcription.txt")
            with open(text_file, 'w', encoding='utf-8') as f:
                f.write(text)
            
        # éªŒè¯åˆ†æ®µæ•°æ®
        self._validate_segment_data(optimized_segments)
        
        # ä¿å­˜ä¼˜åŒ–åçš„åˆ†æ®µï¼ˆä¼˜å…ˆæ˜¾ç¤ºç¿»è¯‘åçš„ä¸­æ–‡ï¼‰
        # æ ¹æ®åˆ†æ®µæ–¹æ³•é€‰æ‹©æ–‡ä»¶å
        segmentation_config = self.whisper_config.get("segmentation", {})
        segmentation_method = segmentation_config.get("method", "rule_based")
        
        if segmentation_method == "punctuation":
            segments_file = os.path.join(output_dir, f"{safe_name}_punctuation_segments.txt")
        else:
            segments_file = os.path.join(output_dir, f"{safe_name}_optimized_segments.txt")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        if output_dir:
            create_output_dir(output_dir)
        
        with open(segments_file, 'w', encoding='utf-8') as f:
            for segment in optimized_segments:
                # ä¿å­˜åŸå§‹åˆ†æ®µæ–‡æœ¬ï¼ˆä¸åŒ…å«ç¿»è¯‘ç»“æœï¼‰
                text_to_save = segment.get('text', '')
                f.write(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {text_to_save}\n")
        
        # ä¿å­˜ç¿»è¯‘ç»“æœåˆ°å•ç‹¬çš„æ–‡ä»¶
        translation_file = os.path.join(output_dir, f"{safe_name}_translation_result.txt")
        with open(translation_file, 'w', encoding='utf-8') as f:
            for segment in optimized_segments:
                if 'translated_text' in segment:
                    f.write(f"[{segment['start']:.2f}s - {segment['end']:.2f}s] {segment['translated_text']}\n")
        
        return {
            "success": True,
            "text": text,
            "language": language,
            "segments": optimized_segments,
            "audio_path": audio_path,
            "text_file": text_file,
            "segments_file": segments_file,
            "translation_file": translation_file,
            "processing_info": {
                "model_size": self.model_size,
                "task": self.task,
                "language_detected": language,
                "segment_count": len(optimized_segments),
                "segment_optimized": self.segment_optimizer is not None
            }
        }
    
    def _validate_segment_data(self, segments: List[Dict[str, Any]]) -> None:
        """éªŒè¯åˆ†æ®µæ•°æ®çš„å®Œæ•´æ€§å’Œä¸€è‡´æ€§"""
        self.logger.info("ğŸ” éªŒè¯åˆ†æ®µæ•°æ®...")
        
        for i, segment in enumerate(segments):
            start = segment.get("start", 0.0)
            end = segment.get("end", 0.0)
            text = segment.get("text", "")
            audio_path = segment.get("audio_path", "")
            
            duration = end - start
            
            self.logger.info(f"åˆ†æ®µ {i}: {start:.2f}s - {end:.2f}s ({duration:.2f}s)")
            self.logger.info(f"  æ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
            self.logger.info(f"  éŸ³é¢‘æ–‡ä»¶: {audio_path}")
            
            # æ£€æŸ¥æ—¶é—´æˆ³åˆç†æ€§
            if end <= start:
                self.logger.error(f"  âŒ æ—¶é—´æˆ³é”™è¯¯: end({end}) <= start({start})")
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if audio_path and not os.path.exists(audio_path):
                self.logger.error(f"  âŒ éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
            
            # æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ä¸ºç©º
            if not text.strip():
                self.logger.warning(f"  âš ï¸ æ–‡æœ¬ä¸ºç©º")
    
    def get_available_models(self) -> List[str]:
        """
        è·å–å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
        
        Returns:
            å¯ç”¨æ¨¡å‹åˆ—è¡¨
        """
        return ["tiny", "base", "small", "medium", "large", "large-v2", "large-v3"]
    
    def get_supported_languages(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        
        Returns:
            æ”¯æŒçš„è¯­è¨€åˆ—è¡¨
        """
        if not WHISPER_AVAILABLE:
            return []
        return list(whisper.tokenizer.LANGUAGES.keys())
    
    def transcribe_with_progress(self, audio_path: str, output_dir: Optional[str] = None,
                                progress_callback: Optional[callable] = None) -> Dict[str, Any]:
        """
        å¸¦è¿›åº¦å›è°ƒçš„è½¬å½•
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
            
        Returns:
            è½¬å½•ç»“æœå­—å…¸
        """
        if progress_callback:
            progress_callback(0.0, "å¼€å§‹è¯­éŸ³è¯†åˆ«...")
        
        # æ‰§è¡Œè½¬å½•
        result = self.transcribe_audio(audio_path, output_dir)
        
        if progress_callback:
            progress_callback(100.0, "è¯­éŸ³è¯†åˆ«å®Œæˆ")
        
        return result
    
    def _save_whisper_word_timestamps(self, whisper_result: Dict[str, Any], audio_path: str, output_dir: str):
        """ä¿å­˜WhisperåŸå§‹å•è¯æ—¶é—´æˆ³åˆ°æ–‡ä»¶"""
        try:
            self.logger.info("ğŸš€ å¼€å§‹ä¿å­˜WhisperåŸå§‹å•è¯æ—¶é—´æˆ³...")
            
            input_name = Path(audio_path).stem
            safe_name = safe_filename(input_name)
            
            self.logger.info(f"è¾“å…¥æ–‡ä»¶å: {input_name}, å®‰å…¨æ–‡ä»¶å: {safe_name}")
            
            # ä¿å­˜åŸå§‹Whisperç»“æœ
            whisper_file = os.path.join(output_dir, f"{safe_name}_whisper_raw.json")
            self.logger.info(f"ä¿å­˜åŸå§‹Whisperç»“æœåˆ°: {whisper_file}")
            
            with open(whisper_file, 'w', encoding='utf-8') as f:
                json.dump(whisper_result, f, ensure_ascii=False, indent=2)
            
            # ä¿å­˜å•è¯æ—¶é—´æˆ³åˆ°å•ç‹¬æ–‡ä»¶
            word_timestamps_file = os.path.join(output_dir, f"{safe_name}_word_timestamps.txt")
            self.logger.info(f"ä¿å­˜å•è¯æ—¶é—´æˆ³åˆ°: {word_timestamps_file}")
            
            with open(word_timestamps_file, 'w', encoding='utf-8') as f:
                f.write("WhisperåŸå§‹å•è¯æ—¶é—´æˆ³:\n")
                f.write("=" * 50 + "\n\n")
                
                segments = whisper_result.get("segments", [])
                self.logger.info(f"æ‰¾åˆ° {len(segments)} ä¸ªåˆ†æ®µ")
                
                for i, segment in enumerate(segments):
                    f.write(f"åˆ†æ®µ {i+1}: {segment.get('start', 0):.2f}s - {segment.get('end', 0):.2f}s\n")
                    f.write(f"æ–‡æœ¬: {segment.get('text', '')}\n")
                    f.write("å•è¯æ—¶é—´æˆ³:\n")
                    
                    words = segment.get('words', [])
                    self.logger.info(f"åˆ†æ®µ {i+1} åŒ…å« {len(words)} ä¸ªå•è¯")
                    
                    for j, word in enumerate(words):
                        f.write(f"  {j+1:2d}. {word.get('word', ''):<15} {word.get('start', 0):6.2f}s - {word.get('end', 0):6.2f}s (æ¦‚ç‡: {word.get('probability', 0):.3f})\n")
                    f.write("\n")
                
                # ç»Ÿè®¡ä¿¡æ¯
                total_words = sum(len(segment.get('words', [])) for segment in segments)
                max_time = max(segment.get('end', 0) for segment in segments) if segments else 0
                f.write(f"\nç»Ÿè®¡ä¿¡æ¯:\n")
                f.write(f"æ€»åˆ†æ®µæ•°: {len(segments)}\n")
                f.write(f"æ€»å•è¯æ•°: {total_words}\n")
                f.write(f"æœ€å¤§æ—¶é—´æˆ³: {max_time:.2f}ç§’\n")
                
                self.logger.info(f"ç»Ÿè®¡ä¿¡æ¯: æ€»åˆ†æ®µæ•°={len(segments)}, æ€»å•è¯æ•°={total_words}, æœ€å¤§æ—¶é—´æˆ³={max_time:.2f}ç§’")
            
            self.logger.info(f"âœ… WhisperåŸå§‹ç»“æœå·²ä¿å­˜: {whisper_file}")
            self.logger.info(f"âœ… å•è¯æ—¶é—´æˆ³å·²ä¿å­˜: {word_timestamps_file}")
            
        except Exception as e:
            self.logger.error(f"âŒ ä¿å­˜WhisperåŸå§‹ç»“æœå¤±è´¥: {e}")
            import traceback
            self.logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
    
    def _save_optimized_segments(self, segments: List[Dict], segments_txt_file: str, segments_json_file: str):
        """
        ä¿å­˜ä¼˜åŒ–åçš„åˆ†æ®µæ–‡ä»¶
        
        Args:
            segments: ä¼˜åŒ–åçš„åˆ†æ®µåˆ—è¡¨
            segments_txt_file: æ–‡æœ¬æ ¼å¼åˆ†æ®µæ–‡ä»¶è·¯å¾„
            segments_json_file: JSONæ ¼å¼åˆ†æ®µæ–‡ä»¶è·¯å¾„
        """
        # ä¿å­˜ä¸ºæ–‡æœ¬æ ¼å¼
        with open(segments_txt_file, 'w', encoding='utf-8') as f:
            for i, segment in enumerate(segments):
                start = segment.get('start', 0)
                end = segment.get('end', 0)
                text = segment.get('text', '').strip()
                f.write(f"[{i+1}] {start:.2f}s - {end:.2f}s: {text}\n")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼
        with open(segments_json_file, 'w', encoding='utf-8') as f:
            json.dump(segments, f, ensure_ascii=False, indent=2)

